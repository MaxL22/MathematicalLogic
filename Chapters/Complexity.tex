% TeX root = ../MathLogic.tex

	\chapter{Notions of computational complexity}

	We want to reason about the \textbf{complexity of a decision problem} (has an answer which is either \textit{yes} or \textit{no}).\\

	\paragraph{Definition:} A (decision) \textbf{problem} (or "\textit{language}") is a subset $L \subseteq \Sigma^\ast$, i.e., a subset $L$ of the set of all finite strings (words) over a finite alphabet (set) $\Sigma$.\\

	For instance, formulas of $\FL$ can be written over the alphabet
	$$ \Sigma = \{\wedge, \vee, \neg, \rightarrow\} \cup \{),(\} \cup \{p, |\} \;\;\;\; p_i \in \mathcal{L} $$

	So we can say that:
	$$ \fl \subseteq \Sigma^\ast $$

	Given a $w \in \Sigma^\ast$, to know if it belongs to $\FL$, so $w \in \fl$, then \textbf{iff} $w$ is a formula, there is an $L$-construction for $w$.\\

	There are other decision problems we've seen, as defined:
	$$ NNF \subseteq \Sigma^\ast,  \;\;\; CNF \subseteq \Sigma^\ast,  \;\;\; DNF \subseteq \Sigma^\ast $$
	These happen to be \textbf{syntactical problem} which are (\textit{usually}) \textbf{easy}.\\

	But there are \textbf{semantics problems}, which are (\textit{usually}) \textbf{not so easy}. For instance:
	$$ SAT \subseteq \Sigma^\ast$$
	Input $w \in \Sigma^\ast$, definition: $w \in SAT$ iff $w \in \fl$ and $w$ is satisfiable.\\
	We can decide if a formula is satisfiable, but it's long (think of a truth table, it's exponential in length).\\

	Another semantical problem:
	$$ TAUTO \subseteq \Sigma^\ast$$
	Same as before, but all entries must be 1, not  at least one.\\

	Same with checking if a formula is unsatisfiable:
	$$ UNSAT \subseteq \Sigma^\ast $$
	and complement of SAT, which is checking if it's unsatisfiable but it doesn't have to be a formula
	$$ SAT^C \subseteq \Sigma^\ast $$

	We can solve these problems, although inefficiently.\\

	\section{Computational Model}
	We want to \textbf{fix a computational model} which will provide a reasonable notion of \textbf{elementary step of computation}.\\

	\paragraph{Church-Turing (extended) Thesis:} There are \textbf{many} different reasonable \textbf{computational models}, but the \textbf{set of computable functions} (what can actually be computed on these models) \textbf{is always the same}. \\
	Every "reasonable" computational model is "equivalent", in the sense that every one of these models defines the same set of efficiently computable functions (decidable problems).\\

	We choose the \textbf{model of Turing Machines}, which comes in several flavors:
	\begin{itemize}
		\item Vanilla: \textbf{Deterministic Turing Machines DTM}
		\item Chocolate: \textbf{Non-Deterministic Turing machines NDTM}
	\end{itemize}

	Which will be useful to define complexity classes.\\

	I'm not going to describe a Turing machine, you got here, you know Turing machines, always the same quintuple $(q,b,q',b',s)$.\\

	\newpage

	The computation goes on until it reaches a state declared (in the definition of the machine) to be final, and accepts the input if this final state is declared to be accepting.\\
	The machine could also never stop, thus not accepting the input, and it could also explicitly reject the input.\\

	%Check here, maybe, idk what's happening

	\textbf{Distinction} between \textbf{deterministic} (vanilla) and \textbf{non-deterministic} (chocolate) Turing machines:
	\begin{itemize}
		\item \textbf{Deterministic} (called DTM): means that for \textbf{any state-symbol pair} $(q,b)$ there is \textbf{at most one instruction} of the form $q,b \rightarrow \dots$.\\

		\item \textbf{Non-Deterministic} (called NDTM): there may be \textbf{several distinct instruction for any state-symbol pair} of the form $q,b \rightarrow \dots$; several instruction for the same state and symbol. When such an instruction is reached, there's a universe for each possible instruction the machine can execute. A DTM is a sequence of steps, a NDTM is a tree of possibilities and an input is accepted when at least one of the leafs in the NDTM tree reaches an accepting state.\\
	\end{itemize}

	\newpage

	\section{Computational Complexity Classes}

	Decision problems which are \textbf{efficiently} decidable.\\

	\paragraph{Class $\mathcal{P}$:} problems decidable in deterministic polynomial time.\\

	A problem $L \subseteq \Sigma^\ast$ is in $\mathcal{P}$ ($L \in \mathcal{P}$) \textbf{iff} there exists a Deterministic Turing Machine $T$ and a polynomial $p: \mathbb{N} \rightarrow \mathbb{N}$ such that for each possible input $w \in \Sigma^\ast$
	\begin{itemize}
		\item the computation of $T$ on input $w$ ($T(w)$) ends within $p(||w||)$ steps (with $||w||$ meaning the length of $w$, the number of characters composing $w$).
		\item for each $w \in L$, $T(w)$ accepts. For each $w \notin L$, $T(w)$ does not accept.
	\end{itemize}

	A polynomial $p$ is a polynomial if it can be expressed as a polynomial.\\

	\paragraph{Class $\mathcal{NP}$:} problems decidable in non-deterministic polynomial time.\\

	\newpage

	\section*{Notion of Efficiency}

	The \textbf{composition of polynomials is a polynomial}, so multiple polynomials together are still a polynomial and thus efficient (but with a \textbf{higher degree}, useful for combining algorithms).\\
	Composition of polynomials of a fixed degree (if greater than 1) is not a polynomial of that degree.\\

	\textbf{Multiplication of polynomials is}, again, \textbf{a polynomial}, but, again, the result is of a \textbf{different degree}.\\

	We want that our notion of "\textbf{efficient computation}" to be \textbf{robust under composition, multiplication and change of the computational model}. So we're lead to \textbf{use polynomial} (with no fixed bound on the degree).\\

	\textbf{TL;DR}: polynomials are efficient.\\

	Previously seen syntactical problems are $\in \mathcal{P}$.\\

	% End L12

	\vfill

	\paragraph{Class $\mathcal{NP}$:} Like $P$ but on NDTM, a problem $L \subseteq \Sigma^\ast$ is $\in \mathcal{NP}$ \textbf{iff} there exists a deterministic Turing Machine $T$ and two polynomials $p,q:\mathbb{N} \rightarrow \mathbb{N}$ such that for all $w \in \Sigma^\ast$ and each $z \in \Gamma^\ast$ ($\Gamma$ may be the same as $\Sigma$) we have:
	\begin{itemize}
		\item $T(w,z)$ ends within $p(||w||)$ steps
		\item For each $w \in L$ there is additional information $z \in \Gamma^\ast$ such that $||z|| \leq q (||w||)$ (is polynomial with respect to the length of $w$) and $T(w,z)$ accepts
		\item For each $w \notin L$ and for all $z \in \Gamma^\ast$ such that $||z|| \leq q(||w||)$, $T(w,z)$ does not accept
	\end{itemize}

	Equivalently: A problem $L$ is in $\mathcal{NP}$ iff there \textbf{exists a NDTM} $T$ and a \textbf{polynomial} $p: \mathbb{N} \rightarrow \mathbb{N}$ such that $w \in L$ iff $T(w)$ accepts in a number of steps $\leq p(||w||)$.\\

	\newpage

	The class $\mathcal{NP}$ as the class of \textbf{polynomially verifiable guess and check procedures}: if $w \in L$, guess an information $z \in \Gamma^\ast$ witnessing that $w$ is actually in $L$. Basically, class of problems in which, given an answer I can check that said answer confirms that $w \in L$.\\
	Check in deterministic polynomial time that $z$ witnesses that $w \in L$.\\

	\paragraph{Example:} SAT, taking $w \in \Sigma^\ast$ with the alphabet:
	$$ \Sigma = \{\wedge, \vee, \neg, \rightarrow\} \cup \{),(\} \cup \{p, |\} \;\;\;\; p_i \in \mathcal{L} $$
	$w \in SAT$ \textbf{iff} $w \in fl$ and $w$ is \textbf{satisfiable}.\\

	Is $SAT \in \mathcal{P}$? Nobody knows (since nobody knows whether $\mathcal{P} = \mathcal{NP}$, we can guess no).\\

	Is $SAT \in \mathcal{NP}$? Looking at the definitions given for the class, can I guess and check? If a relevant truth assignment \textit{descended from above}, could I \textbf{check that assignment in polynomial time}? Of course I can. We can guess a (restricted, obviously) assignment and check in deterministic polynomial time in $||w||$ that the assignment equals 1 (satisfies satisfiability).\\

	\textbf{Clearly} $\mathcal{P} \subseteq \mathcal{NP}$.\\
	What about the other way around? It would mean that $\mathcal{P} = \mathcal{NP}$, and it's part of the millennium problems. You probably ain't solving that.\\

	What about \textbf{other problems}? \\

	$UNSAT \in co-\mathcal{NP}$, i.e., it's in the complement of $\mathcal{NP}$, the set of problems for which I can't guess and check. I can't say that something is $UNSAT$ only from a random assignment.\\
	Same for $SAT \in co-\mathcal{NP}$, $TAUTO \in co-\mathcal{NP}$ (since we can just check that the negation of the input is unsat).\\

	On $\mathcal{NP}$ \textbf{problems} we can do \textbf{guess and check if an answer is correct}, while on $co-\mathcal{NP}$ \textbf{problems} we can't do that, equivalently we can \textbf{guess and check if the answer is NOT correct}.\\


	\paragraph{$\mathcal{NP}$-complete ($\mathcal{NP}c \subseteq \mathcal{NP}$):} we say that $L$ is $\mathcal{NP}c$ \textbf{iff}
	\begin{itemize}
		\item $L \in \mathcal{NP}$
		\item Every $L' \in \mathcal{NP}$ is such that $L'$ is \textbf{polynomially reducible to} $L$ (we say that $L$ is $\mathcal{NP}$-hard)
	\end{itemize}

	\section{Polynomially reducible}

	A problem $L_1 \subseteq \Sigma^\ast$ is \textbf{polynomially reducible} to a problem $L_2 \subseteq \Gamma^\ast$ \textbf{iff} there exists a \textbf{DTM} $T_{L_1, L_2}$ and a polynomial $p : \mathbb{N} \rightarrow \mathbb{N}$ such that for all $w \in \Sigma^\ast$, $T_{L_1, L_2}$ transforms $w$ into $w' \in \Gamma^\ast$ using a number of steps $\leq p (||w||)$ and such that $w \in L_1$ iff $w' \in L_2$. We write $L_1 \preceq_p L_2$.\\

	This means that solving problem $L_2$ means solving $L_1$, each input of one problem can be \textbf{efficiently} (in polynomial time) \textbf{transformed} into an instance of the other.\\

	If $L \in \mathcal{NP}c$ and $L \in \mathcal{P}$ then $\mathcal{NP} = \mathcal{P}$, since every other problem $\in \np$ could be reduced to a problem $\np c$, making $\mathcal{P}$ and $\np$ collapse.\\

	\newpage

	\subsection{Cook-Levin Theorem}

	Basically, it says that there are $\mathcal{NP}c$ problems.\\

	Cook proved this with $CNF-SAT$, so $CNF-SAT \in \mathcal{NP}c$ (and thus SAT, since it's the \textit{deluxe extended version}).\\

	Polynomial reduction \textbf{example:}
	\begin{itemize}
		\item Tauto and UNSAT
		$$ TAUTO \preceq_p UNSAT $$
		$$ w \mapsto \neg w $$
		$$ w \in TAUTO \; \text{ iff } \; \neg w \in UNSAT $$
		C'mon, flip the input and it's the same.\\

		\item CNFSAT to SAT: $w \in \Sigma^\ast$, $w \in CNFSAT$ iff $w \in CNF$ and $w \in SAT$
		$$ CNFSAT \preceq_p SAT $$
		$$ w \mapsto \begin{cases}
			w & \text{ if } w \in CNF \\
			\bot & \text{ if } w \notin CNF
		\end{cases}$$
		\nn
	\end{itemize}


	By Cook-Levin Theorem $CNFSAT \pr SAT$ and $SAT \preceq_p CNFSAT$.\\

	The proof given by Cook of the Cook-Levin theorem is by "simulation".\\

	Given a problem $L \subseteq \Sigma^\ast$, $L \subseteq \mathcal{NP}$, we have to show that $L \preceq_p CNFSAT$. By def of $\mathcal{NP}$ there is a NDTM $T_L$ deciding $L$ in non-deterministic polynomial time.\\

	The idea of the proof is, for each $w \in \Sigma^\ast$ to build a CNF which mimics the computation of $T_L$ on $w$.\\
	Finally, the produced CNF is satisfiable \textbf{iff} $T_L (w)$ accepts.\\

	\newpage

	Since $T_L$ works in polynomial time with respect to $||w||$, a careful analysis of the produced CNF shows that its length is polynomial with respect to $||w||$.\\

	Cook-Levin theorem \textbf{shows that} $L \preceq_p CNFSAT$ \textbf{for any} $L \in \mathcal{NP}$ (equivalent to saying that $CNFSAT \in \mathcal{NP}c$).\\

	There's a big graph of complexity and computability classes here, \textit{I ain't doing all that}.\\

	\nn

	Some \textbf{problems} and their \textbf{relative classes}
	\begin{itemize}
		\item $CNFSAT \pr DNFUNTAUTO \in \np c$.\\

		\item $CNFTAUTO \pr DNFUNSAT \in \mathcal{P}$.\\

		\item $CNFUNTAUTO \pr DNFSAT \in \mathcal{P}$.\\

		\item $DNFTAUTO \pr CNFUNSAT \in co-\np c$.\\

	\end{itemize}

	\paragraph{Questions/Observations: }
	\begin{itemize}
		\item Is it computationally efficient transforming a CNF into a logically equivalent DNF? It's \textit{very unlikey} since it would means reducing CNFSAT to DNFSAT efficiently thus making $\mathcal{P} = \mathcal{NP}$. $CNFSAT \preceq_p DNFSAT \implies \mathcal{P} = \mathcal{NP}$.\\

		\item Is it computationally efficient transforming a DNF into a logically equivalent CNF? No, same as above but with $\mathcal{NP}c \ni DNFUNTAUTO \preceq_p CNFUNTAUTO \in \mathcal{P}$
	\end{itemize}

	Nonetheless, we'll show a poly reduction from SAT to CNFSAT
	$$ SAT \preceq_p CNFSAT $$
	If $SAT \pr DNFSAT$ then $\pnp$.\\

	\newpage

	\section{Equisatisfiability}

	\paragraph{Definition:} Let $A,B \in fl$, then $A$ is equisatisfiable with $B$ ($A$ and $B$ are equisatisfiable) \textbf{iff} $A$ is satisfiable \textbf{iff} $B$ is satisfiable.\\

	That is: either $A$ and $B$ are both satisfiable xor $A$ and $B$ are both unsatisfiable (might not be the same assignments, just same satisfiability level).\\

	%Examples? Complete? Who cares?

	% End L13

	%Continuation of equisatisfiability

	Equisat \textbf{considers only satisfiability}, it doesn't care about logical equivalence (but the latter always implies the former).\\

	\paragraph{Is equisat an equivalence relation?} We need to prove reflexivity, symmetry and transitivity.\\
	So we can confidently say that \textbf{it's an equivalence relation}.\\

	\paragraph{Is equisat a congruence with respect to the operations considered (connectives)?} We can find equisat formulas not always satisfied by all the same assignments (we can prove this for any of the connectives).\\
	So it's \textbf{not a congruence}.\\

	Exercises:
	\begin{itemize}
		\item is equisat a congruence with respect to $\wedge$?
		\item is equisat a congruence with respect to $\vee$?
		\item is equisat a congruence with respect to $\rightarrow$?
	\end{itemize}

	$A \equiv B$ implies that $A$ equisat $B$. $\equiv$ is, as a relation, more refined than equisat (i.e., equisat is rougher than $\equiv$).\\

	The \textbf{equivalence classes of equisat} are \textbf{unions} of \textbf{equivalence classes of} $\equiv$, more specifically, equisat cares only between unsatisfiable and satisfiable (takes the $\bot$ class and the union of all the rest), while $\equiv$ has classes for each assignment.\\

	\newpage

	\subsection{SAT to CNFSAT}
	We shall \textbf{reduce} $SAT$ polynomially $\preceq_p$ to $CNFSAT$ \textbf{using preservation of equisatisfiability} instead of preservation of logical equivalence.\\

	\paragraph{Observation:} Recall that $SAT \preceq_p NNFSAT$, preserving logical equivalence (since transforming a formula to $NNF$ can be done in polynomial time while preserving logical equivalence).\\

	\paragraph{Observation:} If $A \in NNF$ and assume that $A \notin CNF$ then $A$ contains at least one subformula of the following form:
	$$ (\dag) \;\;\; C \vee (D_1 \wedge D_2) \; \text{ or } \;  (D_1 \wedge D_2) \vee C \;\;\; (\dag \dag) $$
	for some $C, D_1, D_2 \in NNF$.\\

	Since $A \in NNF$, $A$ is a $\wedge$-$\vee$ combination of literals. If every occurrence of $\vee$ is in a subformula with the main connective $\wedge$ then $A \in CNF$. If $A \notin CNF$ there must be a subformula of the shape $(\dag)$ or $(\dag \dag)$.\\

	We call any occurrence of $(\dag)$ or $(\dag \dag)$ a "\textbf{violation}".\\
	Preliminary step: since $(\dag) \equiv (\dag \dag)$, we use substitution theorem to \textbf{replace every occurrence of the kind} $(\dag \dag)$ \textbf{with} $(\dag)$. This requires constant time.\\

	We can now assume that \textbf{every violation is in the form} $(\dag)$.\\

	Let $A \in NNF$ and let $B = (\dag)$ with $B \preceq A$. Then $B$ is a violation.\\

	The \textbf{original formula} may contain a \textbf{finite number} $n$ \textbf{of violations}, so we \textbf{eliminate at least one at a time}, while \textbf{maintaining equisatisfiability} and doing this in \textbf{efficient time}, to obtain something equisat.\\

	\newpage

	For \textbf{each violation} $B$ we \textbf{introduce} a new (not occurring in $A$) propositional \textbf{letter} $a_B \in \mathcal{L}$.\\

	We \textbf{define }
	$$ B' := B \left[\sfrac{a_b}{D_1 \wedge D_2}\right] \wedge (\neg \vee D_1) \wedge (\neg a_b \vee D_2) $$
	And we call the part after the first $\wedge$ "\textbf{tail}".\\
	Where
	$$ B'' := \left[\sfrac{a_b}{D_1 \wedge D_2}\right] $$
	is the formula obtained by replacing in $B$ every occurrence of $D_1 \wedge D_2$ with the letter $a_B$.\\

	So
	$$B' = (C' \vee a_b) \wedge (\neg \vee D_1) \wedge (\neg a_b \vee D_2) $$
	with
	$$ C' = C \left[\sfrac{a_b}{D_1 \wedge D_2}\right] $$

	We shall prove that $A$ \textbf{and} $A'$ obtained by \textbf{replacing} $B$ \textbf{in} $A$ \textbf{with} $B'$ are \textbf{equisat} (while, in general, not being $\equiv$).\\

	Examples $p,q,r \in \mathcal{L}$ and a fresh $a \in \mathcal{L}$ ($a \neq p, a \neq q, a \neq r$).\\
	We can show how a simple violation can be removed:
	$$ p \vee (q \wedge r) \mapsto (p \vee a) \wedge (\neg \vee q) \wedge (\neg a \vee r) $$
	The first is not a CNF, while the second is $\in CNF$. We now have to show that they're equisat.\\

	With the assignment
	$$ v(q) = v(r) = v(a) = 1, \;\;\; v(p) = 0$$
	we can see that
	$$ \tilde v (p \vee (q \wedge r)) = 1 $$
	$$ \tilde v ((p \vee a) \wedge (\neg \vee q) \wedge (\neg a \vee r)) = 1$$
	So they're \textbf{both satisfiable} $\implies$ they're \textbf{equisat}.

	\newpage

	But \textit{are they logical equivalent?} Considering
	$$ w(q) = w(r) = 1, \;\;\; w(p) = w(a) = 0$$
	Then
	$$ \tilde w (p \vee (q \wedge r)) = 1 $$
	$$ \tilde w ((p \vee a) \wedge (\neg \vee q) \wedge (\neg a \vee r)) = 0$$

	So they're equisat, but \textbf{not} $\equiv$.\\

	We \textbf{can generalize this process} with not only letters but formulas in general, composed of $n$ literals.\\

	Transforming \textbf{from DNF} of $n$ disjuncts of 2 literals each \textbf{to CNF} while maintaining logical equivalence takes $2^n$ conjuncts, while \textbf{maintaining just equisatisfiability} only \textbf{takes} $2n+1$ (so it's \textbf{linear} instead of exponential).\\

	\paragraph{Algorithm:}
	\begin{itemize}
		\item Let $F \in \fl$.\\

		\item Let $A \in NNF$ such that $A \equiv F$. $A$ is built in polynomial time with respect to $||F||$.\\

		\item Build a sequence
		$$ F \equiv A := A_0 \rightsquigarrow A_1 \rightsquigarrow\, \dots \, \rightsquigarrow A_u \in CNF $$
		where $u \leq$ the number of violations occurring in $A$ ($u \leq ||A||$, max number of steps) and $A_{i+1}$ is obtained from $A_i$ by removing at least one violation as follows: remove a violation $B = C \vee (D_1 \wedge D_2)$ by substituting in $A_i$, $B$ with $B'$: the outcome is $A_{i+1}$. Then we shall guarantee that for each step $A_i$ equisat $A_{i+1}$. \\
		Note that $F \equiv A$ equisat with $A_u \in CNF$.\\
	\end{itemize}

	\newpage

	Notice that in passing \textbf{from} $A_i$ \textbf{to} $A_{i+1}$ (from $B$ to $B'$) we have a \textbf{dilatation of the formulas} $||A_{i+1}|| \geq ||A_i||$. But by \textbf{how much}? Remembering that
	$$ B \rightsquigarrow B' = B \left[\sfrac{a_b}{D_1 \wedge D_2}\right] \wedge (\neg \vee D_1) \wedge (\neg a_b \vee D_2) $$

	So $||B'|| \leq ||B|| + k$ where $k$ is a constant (12 counting all symbols).\\
	So $||A_{i+1}|| \leq ||A_i|| + k$.\\

	If the initial number of violations in $A=A_0$ is $v$, after at most $u \leq v$ steps we have produced $A_u \in CNF$, and $||A_u|| \leq ||A|| + u \cdot k \leq (k+1) ||A||$. It increases by a constant number of symbols $k$, for at most $u$ times.\\

	So the \textbf{algorithm runs in deterministic polynomial time}.\\

	\paragraph{Proving correctness:} To prove the correctness of the algorithm it suffices to prove that \textbf{for each index} $i = 0,1, \, \dots \, , u-1$, $A_i$ \textbf{equisat} $A_{i+1}$, then by the transitivity of equisat $F \equiv A_0$ equisat $A_u \in CNF$, $F$ equisat $A_u$.\\

	Let $B \preceq A_i$ be a violation for $A$. Then \\
	$$ B = C \vee (D_1 \wedge D_2) $$
	$$ B' = B'' \wedge (\neg a_b \vee D_1) \wedge (\neg a_B \vee D_2) \; \text{ with } \; B'' = \left[\sfrac{a_b}{D_1 \wedge D_2}\right] $$

	Then we can say that
	$$
	\begin{cases}
		A & := A_i \left[\sfrac{q}{B}\right] \\
		A_i & := A \left[\sfrac{B}{q}\right] \\
		A_{i+1} & := A \left[\sfrac{B'}{q}\right]
	\end{cases}
	$$
	where $q$ is a fresh propositional letter.\\

	\paragraph{Observation:} Take the tail $(\neg a_b \vee D_1) \wedge (\neg a_B \vee D_2)$, we can see that
	$$  (\neg a_b \vee D_1) \wedge (\neg a_B \vee D_2) \equiv a_b \rightarrow (D_1 \wedge D_2) $$
	So $a_B$ is always $\leq$ than $(D_1 \wedge D_2)$, which is sufficient, but we could force $a_B \leftrightarrow (D_1 \wedge D_2)$, but the tail becomes longer (and I don't care tbh).

	\newpage

	Assume $A_i$ is \textbf{satisfiable} and we prove that $A_{i+1}$ is \textbf{satisfiable too}. Se there's at least a $\ta$ such that $\tilde v (A_i) = 1$ ($v$ exists by assumption that $A_i \in SAT$).\\

	\paragraph{Definition:} the assignment $v_{a_B}: \mathcal{L} \rightarrow \{0,1\}$ such that it's
	$$
	\begin{cases}
		v_{a_B} := v(p) & \text{ for all } p \in \mathcal{L}, p \neq a_B \\
		v_{a_B} := \tilde v (D_1 \wedge D_2) & \text{ otherwise }
	\end{cases}
	$$
	It's well defined.\\

	Notice that $\tilde v_{a_B} (a_B \rightarrow (D_1 \wedge D_2)) = 1$, by the semantics of implication ($\tilde v_{a_B}$ satisfies the tail).\\

	Then $(\dag) = \tilde v_{a_B} (B') = \tilde v_{a_B} (B'')$, by the semantics of $\wedge$, since $B' = B'' \wedge$ tail.\\

	Now
	$$
	\begin{cases}
		B & = \left(B \left[\sfrac{a_B}{D_1 \wedge D_2}\right]\right) \left[\sfrac{D_1 \wedge D_2}{a_B}\right] \\
		B'' & = \left(\left[\sfrac{a_B}{D_1 \wedge D_2}\right]\right) \left[\sfrac{a_B}{a_B}\right]
	\end{cases}
	$$

	Since $a_b$ does not occur in $D_1 \wedge D_2$.\\

	We can now do
	$$ \tilde v_{a_B} (a_B) = v_{a_B} (a_B) = \tilde v (D_1 \wedge D_2) = \tilde v_{a_B} (D_1 \wedge D_2)$$
	Apply to first and last the substitution Lemma:
	$$ \tilde v_{a_B} (B) = \tilde v_{a_B} (B'') \;\;\;\;(\dag \dag)$$

	\newpage

	Now
	\begin{flalign*}
		1 & = \tilde v (A_i) \\
		& = \tilde v_{a_B} (A_i) \\
		& = \tilde v_{a_B} \left(A\left[\sfrac{B}{q}\right]\right) \\
		& = \tilde v_{a_B} \left(A\left[\sfrac{B''}{q}\right]\right) \\
		& = \tilde v_{a_B}  \left(A\left[\sfrac{B'}{q}\right]\right) \\
		& = \tilde v_{a_B} (A_{i+1})
	\end{flalign*}

	And each equivalence, in order, is:
	\begin{itemize}
		\item by current assumption (of smth, idk)
		\item $a_B$ doesn't occur in $A_i$
		\item by definition of $A_i$
		\item by substitution lemma and $(\dag \dag)$
		\item by substitution lemma and $(\dag)$
		\item by definition of $A_{i+1}$
	\end{itemize}

	So $\tilde v_{a_B} (A_{i+1}) = 1$, and thus $A_{i+1} \in SAT$.\\



	Assume now, \textbf{for the other way around}, that $A_{i+1} \in SAT$ with the aim of proving that $A_i$ is $SAT$ too.\\

	Two cases:
	\begin{itemize}
		\item \textbf{First, lucky, case:} Assume further that $A_{i+1}$ is satisfied by an assignment $w: \mathcal{L} \rightarrow \{0,1\}$ of the form $\tilde v_{a_B}$, that is
		$$ w(a_b) = \tilde w (D_1 \wedge D_2) $$
		at least one satisfying assignment has this property. Then we \textit{kinda} reverse the earlier chain:
		\begin{flalign*}
			1 & = \tilde w (A_{i+1}) \\
			& = \tilde w \left(\left[\sfrac{B'}{q}\right]\right) \\
			& = \tilde w \left(\left[\sfrac{B''}{q}\right]\right) \\
			& = \tilde w \left(\left[\sfrac{B}{q}\right]\right) \\
			& = \tilde w (A_i)
		\end{flalign*}
		So $\tilde (A_i) = 1$, that is $A_i \in SAT$.\\

		\item \textbf{Second, unlucky, case:} $A_{i+1}$ is satisfiable but all satisfying assignments $w: \mathcal{L} \rightarrow \{0,1\}$ ($\tilde w (A_{i+1}) = 1$) are such that $\tilde w(a_b) \neq \tilde w (D_1 \wedge D_2)$.\\
		The condition
		$$ \tilde w(a_b) \neq \tilde w (D_1 \wedge D_2) $$
		implies that
		$$ w(a_B) = 0 \; \text{ and } \; \tilde w (D_1 \wedge D_2) = 1$$
		or
		$$ w(a_B) = 1 \; \text{ and } \; \tilde w (D_1 \wedge D_2) = 0$$
		By bivalence, but the second one can be ruled out since $\tilde w$ satisfies the tail, which means that $w (a_B \rightarrow (D_1 \wedge D_2)) = 1$.\\

	\end{itemize}

	\newpage

	%Smth missing I guess

	%Wtf
%	Exercise/Digression: Let $E$ be an expression formed using only $0,1,\min,\max$. Observations:
%	* the value of $E$ is either 0 or 1
%	* Let $E'$ be obtained by replacing zero or more occurrences of 0 in $E$ with 1, then $E \leq E'$

	%End L14

%%% Local Variables:
%%% TeX-master: "../MathLogic.tex"
%%% End:

%Page 1, finish proof of A_i+1 SAT \implies $A_i$ SAT 

\begin{remark}
	This technique ro reduce $SAT \pr CNFSAT$ is inspired to Karp's technique to prove that 
	$$ SAT \pr 3-CNFSAT $$
	Where by $k-CNFSAT$ we mean the problem of deciding satisfiability of CNFs where each clause has exactly/at most $k$ literals each.\\
	
	Karp proved that $3-CNFSAT$ is already $\in \np c$. He also proved that $2-CNFSAT \in \mathcal{P}$.\\
	These are called \textbf{dichotomy results}, up to a certain degree the problem is "easy", then it becomes difficult.\\
\end{remark}

We want to deal with \textbf{problems of the kind} SAT, TAUTO or $\Gamma \models^? A$.\\
About these problems we know that: 
\begin{itemize}
	\item $SAT \pr CNFSAT \in \np c$
	\item $TAUTO \pr UNSAT \pr CNFUNSAT \in co-\np c$
\end{itemize}

Given the problem of \textbf{deciding} whether $\Gamma \models A$ (for $\Gamma$ a finite theory)
$$ \Gamma = \{\gamma_1, \, \dots \, , \gamma_n\} \;\;\; \gamma_i \in \FL, \;\;\; A \in \FL $$

We can \textbf{reduce it} to a problem of CNFUNSAT:
\begin{flalign*}
	\Gamma \models A & \Leftrightarrow \Gamma \cup \{\neg A\} \;\;\; UNSAT (\text{ as a theory}) \\
	& \Leftrightarrow \gamma_1 \wedge \gamma_2 \wedge \, \dots \, \wedge \gamma_u \wedge \neg A \;\;\; UNSAT (\text{ as a formula})\\
	& \Leftrightarrow S \in CNF, \;\; S \in CNFUNSAT \\
\end{flalign*}
For some $S$ equisat with $\gamma_1 \wedge \gamma_2 \wedge \dots \wedge \gamma_n \wedge \neg A$.\\

\newpage

\section{Notational Variant for CNFs}

Take a \textbf{family of formulas}
$$ F_i \in \fl \;\;\;\; i = 1, \, \dots \, , k $$

Then we can "\textit{forget}" to parenthesize these expressions due to \textbf{associativity}
$$ 
\begin{cases}
	F_1 \wedge F_2 \wedge \, \dots \, \wedge F_k \\
	F_1 \vee F_2 \vee \, \dots \, \vee F_k
\end{cases}
$$

We can also forget about order, due to \textbf{commutativity}
$$
\begin{cases}
	F_1 \wedge F_2 \equiv F_2 \wedge F_1  \\
	F_1 \vee F_2 \equiv F_2 \vee F_1 
\end{cases}
$$

And repetitions, for \textbf{idempotence}:
$$ 
\begin{cases}
	F_1 \wedge F_1 \equiv F_1 \\
	F_1 \vee F_1 \equiv F_1
\end{cases}
$$

So from now on a \textbf{clause} $l_1 \vee l_2 \vee \, \dots \, \vee l_u$ will be denoted as the \textbf{set of its literals} $\{l_1, l_2, \, \dots \, , l_u\}$.\\
A \textbf{CNF} $C_1 \vee C_2 \vee \, \dots \, \vee C_u$ will be denoted by the \textbf{set of its clauses}  $\{C_1, C_2, \, \dots \, , C_u\}$.\\

A CNF will be a set of sets of literals. A theory made of CNFs will be a set of sets of literals.\\

\marginnote{What? It says the same thing? Also on the lecture notes? TOASK}[-1cm]

\textbf{Example}:
$$
\begin{array}{c c}
	CNF: & (p \vee q \vee \neg r) \wedge (q \vee r \vee a) \wedge p \\
	\downarrow & \\
	CNF: & \left\{\{p, q, \neg r\}, \{q,r,a\}, \{p\}\right\} 
\end{array}
$$

A theory of CNFs is just a set of clauses. An infinite theory is a set containing infinitely many clauses.\\

Now the notation of empty CNF as $\emptyset$ \textit{makes sense} (thinking about an empty set of clauses), while an empty clause is $\square$ (empty set of literals).\\

A CNF containing an \textbf{empty clause} is \textbf{unsatisfiable}.\\
The shortest CNF containing $\square$ is just $\{\square\}$.\\
Also the empty CNF $\emptyset$ is SAT, even tautological.\\

\newpage

We want to deal with $\Gamma \models^? A$. We want to reduce it to the problem of deciding whether a \textbf{set of clauses} $S$ (possibly infinite) is \textbf{unsatisfiable}.\\

$S$ is SAT \textbf{iff} there is an assignment $\ta$ such that $v \models C$ for each $C \in S$ (satisfies each clause in the set). For all $C \in S$ there is at least a literal $L \in C$ such that $v \models L$ ($\tilde v (L) = 1$, it satisfies the literal).\\

If we want to prove that $S$ is $UNSAT$, a good strategy is to \textbf{enlarge} $S$ into a new set $S'$ ($S \subseteq S'$) in such a way that $S \equiv S'$ (or at least $S$ equisat $S'$).\\

For instance, if enlarging $S$, iteratively 
$$ S \subseteq S' \subseteq S'' \, \dots \, \subseteq S^{(k)}$$
if we find that $\square \in S^{(k)}$, then we can conclude that $S^{(k)}$ is UNSAT, so $S$ is UNSAT too.\\

\newpage

\section{Refutational Methods}

They are \textbf{deductive methods} whose aim is to \textbf{prove the unsatisfiability of a set of clauses} (of a formula or of a theory).\\

Many of these methods are based on the inference rule called "\textit{resolution principle}".\\

They are calculi particularly \textbf{fit to be automated} (for proof search, we can just give theory and formula to a machine and make it decide). This is the basic behind Automated Deduction.\\ 

There are \textbf{different possible branches}:
\begin{itemize}
	\item SAT Solver: decide if a given set of clauses is satisfiable or not.\\
	
	\item Theorem Prover: designed to automatically prove mathematical theorem.\\
	
	\item Logic Programming: asking a machine to refute a theorem $\in CNF$ (Prolog).\\
\end{itemize}

\newpage

\subsection{Resolution Principle}
Given two clauses $C_1$ and $C_2$ we say that the clause $D$ is the \textbf{resolvent} of $C_1$ and $C_2$ on the pivot $L$ (where $L$ is a literal) \textbf{iff}:
\begin{itemize}
	\item $L \in C_1$
	\item $\overline L \in C_2$
	\item $D:= (C_1 \setminus \{L\}) \cup (C_2 \setminus \{\overline L\})$
\end{itemize}

We shall write $D = \mathbb{R} (C_1, C_2; L, \overline L)$.\\

Examples
$$ S := \left\{\{x,y,\neg t\}, \{u, \neg y, t\}\right\} $$
How many resolvents can we have? 2, namely: 
$$ D_1: \mathbb{R} (C_1, C_2; y, \neg y) = \{x, \neg t, u, t\} $$
$$ D_2: \mathbb{R} (C_1, C_2; \neg t, t) = \{x,y,u, \neg y\} $$

\begin{lemma}
	\textbf{Correctness} (of the resolution principle, also called "Soundness"). \\
	Let $D = \mathbb{R}(C_1, C_2; L, \overline L)$ be the resolvent of some clauses, then 
	$$\{C_1, C_2\} \models D$$
\end{lemma}

\begin{proof}
	We must show that for each assignment $\ta$, if $v \models C_1$ and $v \models C_2$ then $v \models D$ too. Let $\ta$ be such that $v \models C_1$ and $v \models C_2$.\\
	
	Then $v \models M$ and $v \models N$ for literals $M \in C_1$ and $N \in C_2$. If it were the case that $M=L$ and $N = \overline L$ then $\tilde v (L) = 1$ and $\tilde v(\overline L) = 1$ but this is clearly a contradiction.\\
	
	Then at least one in $M$ or $N$ is such that $M \neq L$ or $N \leq \overline L$. Then we assume without loss of generality that $M \neq L$ then $M \in D$ (by definition of $D$), so $\tilde v (M) = 1$ implies that $\tilde v (D) = 1$.\\
\end{proof}

\begin{corollary}
	If $\mathbb{R} (C_1, C_2; L, \overline L) = \square$ then $\{C_1, C_2\}$ is UNSAT.\\
\end{corollary}

\begin{proof}
	$\{C_1, C_2\} \models \square$ by Correctness Lemma.\\
\end{proof}

\begin{corollary}
	Let $S$ be a set of clauses. Let $S = S_0, \, \dots \, , S_k$ be a sequence of sets of clauses such that:
	\begin{enumerate}
		\item For all indexes $i = 0, 1, \, \dots \, , k-1$, $S_{i+1}$ is obtained from $S_i$ by adding one or more resolvents of clauses in $S_i$
		\item $\square \in S_k$
	\end{enumerate}
	
	Then $S$ is UNSAT.\\
\end{corollary}

\begin{proof}
	$S = S_0 \models S_1 \models S_2 \models \, \dots \, \models S_k \ni \square$, the sequence is guaranteed by correctness Lemma, so $S \models \square$ and $S$ is UNSAT.\\
\end{proof}

\newpage

%\section{Completeness}

Our aim is to show that the \textbf{refutational method} based on iterated applications of resolution principle is \textbf{correct} and \textbf{refutationally complete}.\\

In general, a logical calculus may have (or not) the two following \textbf{properties}:
\begin{itemize}
	\item \textbf{Correctness (Soundness):} a calculus is correct (sound) if the "\textit{certificates}" it outputs (the proofs) witness a true state of things (it does not produce "fake certificates", falsifying assignment). In our case the sequence $S_0, S_1, \, \dots \, , S_k \ni \square$ is a correct "certificate", or proof, that $S$ is UNSAT.\\
	
	\item \textbf{Completeness:} a calculus is complete if it does not omit any "certificate" (proof). In our case it means that to be complete our method should have the following property: If $S$ is UNSAT then there is a sequence (certificate) $S = S_0, \, \dots \, , S_k \ni \square$ ($S_i \rightsquigarrow S_{i+1}$ only adding resolvents from clauses in $S_i$).\\
\end{itemize} 


Our refutation, resolution-based, method we'll only be \textbf{refutationally complete}.\\

\subsection{Refutational Completeness of the Resolution Principle (proof of J.A. Robinson)}

%Check R, change cal?
A (possibly infinite) set of clauses $S$ is unsatisfiable iff (since we're proving correctness and completeness) $\square \in \mathcal{R}^\ast (S)$ where $\mathcal{R}^\ast (S)$ is defined as follows:
$$ \begin{array}{c c}
	\mathcal{R}(S) & := S \cup \{D : D = \mathbb{R} (C_1, C_2; L, \overline L) \text{ for some } C_1, C_2 \in S, L \in C_1, \overline L \in C_2 \} \\
	\mathcal{R}^2 (S) & := \mathcal{R} \left(\mathcal{R} (S)\right) \\
	\vdots & \\
	\mathcal{R}^{t+1} (S) & := \mathcal{R} \left(\mathcal{R}^t (S)\right) \\
	\vdots & \\
	\mathcal{R}^\ast (S) & := \bigcup_{i \in \omega} \mathcal{R}^i (S)
\end{array}
$$

For simplicity
$$ \mathcal{R}^1 (S) := \mathcal{R} (S) $$
$$ \mathcal{R}^0 (S) := S $$

\begin{remark}
	Assume we proved Robinson's theorem. If $S$ is a finite set of clauses, then $\mathcal{R}^\ast$ provides us with a decision procedure for establishing whether $S$ is SAT or UNSAT. That is, in both cases, the procedure building $\mathcal{R}^\ast (S)$ ends in a finite number of steps (finite amount of time) providing always the correct answer.\\
\end{remark}

\begin{proof}
	If $S$ is finite then in $S$ there occur only finitely many distinct propositional letters. Let us write $Var (S) \subseteq \{p_1, p_2, \, \dots \, , p_n\}$ for some $n \in \mathbb{N}$.\\
	
	The literals writable using only $p_1, p_2, \, \dots \, , p_n$ are exactly $2^n$.\\
	The clauses writable using only literals on $p_1, p_2, \, \dots \, , p_n$ are exactly $2^{2n}$.\\
	
	\textbf{Key observation}: the application of the resolution rule (the \textbf{formation of new resolvents}) \textbf{never introduces any new literals}.\\
	
	So, the sequence
	$$(\dag) \;\;\;\; S = \mathcal{R}^0 (S) \subseteq \mathcal{R}^1 (S) \subseteq  \, \dots \, \subseteq \mathcal{R}^k (S) \subseteq \, \dots $$
	is such that every $\mathcal{R}^i (S)$ is a subset of the $2^{2n}$ clauses writable on $p_1, p_2, \, \dots \, , p_n$.\\
	
	Let us write $\mathcal{C}^{(n)}$ for the set of all clauses on $p_1, \, \dots \, , p_n$
	$$ \mathcal{R}^i (S)  \subseteq \mathcal{C}^{(n)} \; \text{ for each } \; i \in \omega $$
	
	Then there must be $t \in \omega$ such that $\mathcal{R}^t (S) = \mathcal{R}^{t+1} (S)$ (since no $\mathcal{R}^i (S)$ can have more that $2^{2n}$ clauses, $t \leq 2^{2n}$).\\
	
	Then 
	$$ \mathcal{R}^t (S) = \mathcal{R}^{t+1} (S) = \mathcal{R} \left(\mathcal{R}^t(S)\right) = \mathcal{R} \left(\mathcal{R}^{t+1} (S)\right) = \mathcal{R}^{t+2} (S) = \, \dots $$
	So we have that 
	$$ \mathcal{R}^t (S) = \mathcal{R}^{t+1} (S)  = \, \dots \, = \mathcal{R}^\ast (S) $$
	
	Then: At step $i \rightsquigarrow i+1$ of the construction: 
	\begin{itemize}
		\item if we found $\square \in \mathcal{R}^{i+1} (S)$ then stop $\implies$ output $S$ UNSAT
		\item If $\square \notin \mathcal{R}^{i+1} (S)$ then 
		\begin{itemize}
			\item if $\mathcal{R}^{i+1} (S) = \mathcal{R}^i (S)$ then stop and output $S$ SAT
			\item else go to the next step
		\end{itemize}
	\end{itemize}
	
	This algorithm \textbf{always terminates when $S$ is finite}.\\
\end{proof}

\textbf{Note:} If $S$ is \textbf{infinite}, but it happens that $\mathcal{R}^t (S) = \mathcal{R}^{t+1} (S)$ (and we can check this), the algorithm can stop and say that $S$ is SAT. The procedure is complete, but it's a semi-decision procedure, might not end.\\

% End L15

%All $C$ clauses must be capitalized

%We've done the correctness (soundness) part, so we proved that 
%$$ \square \in \mathcal{R}^\ast (S) \implies S \;\; UNSAT$$
%
%Now we do \textbf{Completeness}
%$$ S \;\; UNSAT \implies \square \in \mathcal{R}^\ast (S) $$

\subsubsection{Correctness}
Assuming that $\square \in \mathcal{R}^\ast (S)$, then, by definition of $\mathcal{R}^\ast (S) = \bigcup_{i \in \omega} \mathcal{R}^i (S)$, there is $i \in \omega$ such that $\square \in \mathcal{R}^i (S)$.\\

So $\mathcal{R}^i (S)$ is UNSAT, by definition of $\square$.\\
But $\mathcal{R}^i (S) \equiv \mathcal{R}^{i-1} (S) \equiv \, \dots \, \equiv \mathcal{R}^0 (S) \equiv S$ (by Correctness Lemma). So $S$ is UNSAT.\\

\newpage

\subsubsection{(Refutational) Completeness}
We have to prove that $S$ UNSAT implies $\square \in \mathcal{R}^\ast (S)$.\\

\begin{proof}
	Since $S \in UNSAT$ (by assumption), by compactness theorem, there is a $S_{fin} \finsat S$ such that $S_{fin}$ is UNSAT.\\
	
	In $S_{fin}$ there occur finitely many (distinct) propositional letters, say $Var (S_{fin}) \subseteq \{p_1, \, \dots \, , p_n\} \subseteq \mathcal{L}$, for some $n \in \omega$.\\
	
	\begin{remark}
		$S_{fin} \subseteq \mathcal{C}^{(n)}$ (we call $\mathcal{C}^{(n)}$ the set of all clauses over the letters $p_1, \, \dots \, , p_n$).\\
	\end{remark}
	
	\begin{remark}
		$\mathcal{C}^{(0)} = \{\square\}$ ($|\mathcal{C}^{(0)}| = 2^{2 \cdot 0} = 1$).\\
	\end{remark}
	
	A fortiori ("from stronger reasons"): we can say that 
	$$ 
	\left.
	\begin{array}{c c}
		S_{fin} \subseteq &\mathcal{C}^{(n)} \\
		S_{fin} \subseteq & S
	\end{array}
	\right\} \implies S_{fin} \subseteq \mathcal{C}^{(n)} \cap \mathcal{R}^\ast (S)
	$$
	(since $S \subseteq \mathcal{R}^\ast (S)$).\\
	
	It follows 
	$$ (\dag \dag) \;\;\;\;\; \mathcal{C}^{(n)} \cap \mathfrak{R}^\ast (S) \; \text{ is } UNSAT $$
	Since it contains $S_{fin}$, which is UNSAT by definition.\\
	
	We shall prove that for each $k = n, n-1, \, \dots \, , 0$ that $\mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$ is UNSAT $(\dag)$.\\
	
	If we can prove $(\dag)$ for all $k$, then $(\dag)$ holds for $k = 0$ too. That is $\mathcal{C}^{(0)} \cap \mathcal{R}^\ast (S)$ is UNSAT, and thus $\{\square\} \cap \mathcal{R}^\ast (S)$ is UNSAT.\\
	
	If we prove $\{\square\} \cap \mathcal{R}^\ast (S)$ is UNSAT $(\dag\dag\dag)$ then 
	$$ \{\square\} \cap \mathcal{R}^\ast (S) = \begin{cases}
		\emptyset & \rightarrow \{\square\} \cap \mathcal{R}^\ast (S) \; SAT \; \text{ contradicting } \; (\dag\dag\dag) \\
		\text{ or } & \\
		\{\square\} & \rightarrow \{\square\} \cap \mathcal{R}^\ast (S) \; UNSAT \\
	\end{cases}
	$$
	but 
	$$ \{\square\} \cap \mathcal{R}^\ast (S) = \{\square\} \; \text{ iff } \; \square \in \mathcal{R}^\ast (S)$$
	And this is the end of completeness.\\
\end{proof}

\newpage

We shall prove by \textbf{descending induction} on the index $k = n, n-1, \, \dots \, , 0$ that $(\dag)$: $\mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$ is UNSAT.\\

\paragraph{Base:} $k=n$, then $\mathcal{C}^{(k)} \cap \mathcal{R}$ UNSAT, which is already proven, by $(\dag\dag)$.\\

\paragraph{Step:} Assume the statement $(\dag)$ is true for $k=n, k=n-1, 1, \, \dots \, , k+1$ and we want to prove it for $k$. \\
So $\mathcal{C}^{(n)} \cap \mathcal{R}^\ast (S)$ UNSAT, $\mathcal{C}^{(n-1)} \cap \mathcal{R}^\ast (S)$ UNSAT, \dots  $\mathcal{C}^{(k+1)} \cap \mathcal{R}^\ast (S)$ UNSAT, and we want to \textbf{prove} $\mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$ UNSAT, too.\\

By \textbf{contradiction}: we assume $\mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$ is SAT.\\
Then there is $\ta$ such that for all $C \in \mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$, $v \models C$ ($\tilde v(C) = 1$).\\

Let us define the two following variants $v^+, v^- : \mathcal{L} \rightarrow \{0,1\}$ of $v$: 
\begin{itemize}
	\item $v^+ (p_{k+1}) := 1$
	\item $v^- (p_{k+1}) := 0$
	\item $v^+ (p_i) := v(p_i) =: v^- (p_i)$ for all $i \neq k+1$
\end{itemize}

By I.H. $\mathcal{C}^{(k+1)} \cap \mathcal{R}^\ast (S)$ is UNSAT, then $v, v^+, v^- \not \models \mathcal{C}^{(k+1)} \cap \mathcal{R}^\ast (S)$.\\

Then there is at least a clause $C_1 \in \mathcal{C}^{(k+1)} \cap \mathcal{R}^\ast (S)$ such that $\tilde v^+ (C_1) = 0$ ($v^+ \not \models C_1$) $(\star)$, analogously there must be a $C_2 \in \mathcal{C}^{(k+1)} \cap \mathcal{R}^\ast (S)$ such that $\tilde v^- (C_2) = 0$ ($v^- \not \models C_2$) $(\star \star)$.\\

\textbf{Key Observation}: $p_{k+1}$, as a propositional letter, occurs in $C_1$ and, precisely, only in the literal $\neg p_{k+1}$ (same thing but not negated in $C_2$)
$$ \neg p_{k+1} \in C_1, \;\;\; p_{k+1} \notin C_1 $$

\begin{proof}
	As a matter of fact 
	\begin{itemize}
		\item $p_{k+1}$ cannot occur in $C_1$ as a literal, for otherwise $v^+ (p_{k+1}) = 1$ implies $\tilde v^+ (C_1) = 1$, contradicting $(\star)$
		\item $p_{k+1}$ must occur in $C_1$ in the literal $\neg p_{k+1}$, for otherwise $p_{k+1} \notin C_1$, $\neg p_{k+1} \notin C_1$, so $C_1 \in \mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$, and so $\tilde v (C_1) = 1$, but since $p_{k+1}$ does not occur as a variable in $C_1$, $\tilde v^+ (C_1) = \tilde v (C_1) = 1$ that is $\tilde v^+ (C_1) =1$, contradicting $(\star)$.\\
	\end{itemize}
	
	So $\neg p_{k+1} \in C_1$, $p_{k+1} \notin C_1$.\\
\end{proof}

Analogously: $p_{k+1}$ must be in $C_2$ in the form $p_{k+1}$.\\

\newpage

Then $\neg p_{k+1} \in C_1$, $p_{k+1} \notin C_1$, $p_{k+1} \in C_2$, $\neg p_{k+1} \notin C_2$.\\

Then there \textbf{exists the resolvent} 
\begin{flalign*}
	D & = \mathbb{R} (C_1, C_2; \neg p_{k+1}, p_{k+1})\\
	& = (C_1 \setminus \{\neg p_{k+1}\}) \cup (C_2 \setminus \{p_{k+1}\})\\
	& = (C_1 \cup C_2) \setminus \{p_{k+1}, \neg p_{k+1}\}
\end{flalign*}

Notice that $D$ is such that $\neg p_{k+1} \notin D$ and $p_{k+1} \notin D$, so 
$$ D \in \mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S) \implies \tilde v (D) = 1 $$

Then $v$ must satisfy at least one literal in $m \in D$, ($\tilde v (M) = 1$).\\

So $M \in D = (C_1 \cup C_2) \setminus \{p_{k+1}, \neg p_{k+1}\}$, and $\tilde v(M) = 1$, so we have two cases: 
\begin{enumerate}
	\item $M \in C_1 \setminus \{ \neg p_{k+1}, p_{k+1}\}$, then $\tilde v (C_1) = 1$, and then since $p_{k+1}$ does not occur in $C_1$ (as a letter), $\tilde v^+ (C_1) = \tilde v (C_1) = 1$, contradicting $(\star)$.\\
	
	\item $M \in C_2 \setminus \{p_{k+1}, \neg p_{k+1} \}$, then  $\tilde v (C_2) = 1$, and then since $p_{k+1}$ does not occur in $C_2$ (as a letter), $\tilde v^- (C_2) = \tilde v (C_2) = 1$, contradicting $(\star \star)$.\\
\end{enumerate}

As there are no more cases to consider we have concluded our proof by contradiction, showing that it is not the case that $\mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$ is SAT, implying, in any case, that $\mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$ is UNSAT.\\

This concludes induction step and proof by induction. We have proved that $\mathcal{C}^{(k)} \cap \mathcal{R}^\ast (S)$ is UNSAT for all $k = n, \, \dots \, , 0$.\\
%End proof of Completeness theorem



\begin{remark}
	Say $S$ is an \textbf{infinite theory}. In the first step of the proof we reduced $S$ to $S_{fin} \finsat S$ by compactness theorem.\\
	
	This application of compactness theorem doesn't provide us with a decision procedure for the input $S$, when $S$ is infinite, since, in general, we don't know anything useful on the concrete nature of $S_{fin}$ (we have no clues on how to choose such $S_{fin} \finsat S$, $S_{fin}$ UNSAT).\\
	
	The best thing we can do is to \textbf{build a possibly infinite sequence of finite subsets} of $S$:
	$$ 
	S_1 \finsat S_2 \finsat \, \dots \, \finsat S \; \text{ s. th. } \; \bigcup_{i \in \omega} S_i = S
	$$
	
	Then, for each $S_i$, which are all finite, compute $\mathcal{R}^\ast (S_i)$.\\
\end{remark}

\newpage

\begin{remark}
	If $S$ is infinite, at step $i$:
	\begin{itemize}
		\item If $\square \in \mathcal{R}^m (S_i)$ for some $m \in \omega$:
		$$ 
		\begin{array}{c c}
			\implies & \square \in \mathcal{R}^\ast (S_i) \\
			\implies & \square \in \mathcal{R}^\ast (S) \\
			\implies & S \;\;\; UNSAT
		\end{array}
		$$
		\item If we find $t \in \omega$, $\square \notin \mathcal{R}^t (S_i)$ and $\mathcal{R}^t (S_i) = \mathcal{R}^{t+1} (S_i)$, then $S_i$ is SAT, go to the next step $i+1$
	\end{itemize} 
	
	If $S_i$ UNSAT (which is decidable) then $S$ UNSAT and stop, but if $S_i$ is SAT (also decidable) then go on. This is a semi-decidability procedure for "$S$ SAT?".\\
\end{remark}

%Terminology.\\

\begin{definition}
	A \textbf{deduction by resolution} of a clause $C$ from a set of clauses $S$ ($S \vdash_R C$) is a finite sequence $C_1, C_2, \, \dots \, , C_u$ of clauses such that
	\begin{enumerate}
		\item $C_u = C$
		\item For all indexes $i \in \{1, \, \dots \, , u\}$ either
		\begin{enumerate}
			\item $C_i \in S$
			\item $C_i = \mathbb{R} (C_j, C_k ; L, \overline L)$ for some $j,k < i$ (and some literals $L \in C_j$, $\overline L \in C_k$) 
		\end{enumerate}
	\end{enumerate}
	\nn
\end{definition}

$S \vdash_R C$ by correctness, implies that $S \models C$.\\

A \textbf{deduction by resolution of} $\square$ from $S$ ($S \vdash_R \square$) is called a \textbf{refutation} of $S$ (and then $S$ is UNSAT, i.e. $S \models \square$).\\

The \textbf{refutational completeness} of $\vdash_R$ \textbf{shows} 
\begin{itemize}[label*=]
	\item \phantom{equivalently} $S$ UNSAT iff $S \vdash_R \square$
	\item equivalently $S \models \square$ iff $S \vdash_R \square$
	\item equivalently $S \models \bot$ iff $S \vdash_R \bot^c$ %Check
\end{itemize}

A calculus $C$ is \textbf{refutationally complete iff} $\Gamma \models \bot$ implies $\Gamma \vdash_C \bot$ (whenever $\Gamma$ is contradictory, i.e., is UNSAT, then the calculus $C$ proves such contradiction).\\

A calculus $C$ is \textbf{"tout-court"/fully complete} (just complete) \textbf{iff} $\Gamma \models A$ implies $\Gamma \vdash_C A$ for any $A \in \fl$.\\

\newpage

\textbf{Examples of deductions} (not necessarily refutations) in $\vdash_R$

$$ a,b,c \in \mathcal{L} \;\;\; S = \{\{a,b,\neg c\}, \{a,b,c\}, \{a, \neg b\} \} \models^? \{\{a\}\}$$

Steps: 
\begin{enumerate}
	\setcounter{enumi}{-1}
	\item Transform $S \models^? \{a\}$ into $S \cup \{\neg a\}$ UNSAT? That is, show that $S, \neg a \vdash_R \square$
	\item Choose a formula from $S$, $\{a,b, \neg c\} \in S$
	\item Choose another formula? $\{a,b,c\}$
	\item Choose $\{a, \neg b\}$
	\item Choose $\{\neg a\}$
	\item Resolve 1 and 2 together: $\mathbb{R}(1,2)$: $\{a, b\}$
	\item Resolve 3 and 5 together: $\mathbb{R}(3,5)$: $\{a\}$
	\item Resolve 4 and 6 together: $\mathbb{R}(4,6)$: $\square$
\end{enumerate}
%Wtf

%TODO From here, Wtf

The first 6 steps constitute a direct deduction from $S, \neg a \vdash_R \{a\}$, a deduction by resolution of $A$ from $S$.\\

Consider a theory $\Gamma \subseteq \fl$, a formula $A \in \fl$.\\
Let $\Gamma^C$ be the set of clauses equisatisfiable with $\Gamma$. Let $A^C$ ($(\neg A)^C$) be a set of clauses equisat with $A$ ($\neg A$).\\

$$ \Gamma \models A \Leftrightarrow \Gamma, \neg A \;\; UNSAT \Leftrightarrow \Gamma^c, (\neg A)^C \vdash_R \square \Leftarrow \Gamma^C \vdash_R A^C$$

full completeness fails for $\vdash_R$.\\

%Da dove cazzo arrivano le doritos diomerda

%Check
$(\dag)$

So $(\dag)$ shows that $\vdash_R$ is only refutationally complete, that is 
$$ \Gamma \models \bot \Leftrightarrow \Gamma \vdash_C \bot $$
$$ (S \models \square \Leftrightarrow S \vdash_R \square) $$
but it's not complete (so $ $%Complete$)

This is not a problem from the computational point of view, because, via a suitable and efficient "pre-processing", we reduce $\Gamma \models^? A$ to a refutational problem $\Gamma^C, (\neg A)^C \vdash_R^? \square$.\\

Actually, from the computational point of view, calculi that are just refutationally complete, may be preferable for efficiency reasons with respect to fully complete calculi.\\

%Exercise last page, I guess

% End L16
